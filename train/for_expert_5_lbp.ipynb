{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model.expert_5_lbp import MyModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行gpu的设置\n",
    "tf.debugging.get_log_device_placement()  # 会将运算属于哪个gpu给打印出来\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")  # 物理gpu\n",
    "tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")  # 设置某个物理gpu为可见，即为逻辑gpu\n",
    "# tf.config.experimental.set_virtual_device_configuration(  # 对GPU进行逻辑拆分\n",
    "#     gpus[0],\n",
    "#     [\n",
    "#         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),\n",
    "#         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),\n",
    "#     ]\n",
    "# )\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)  # 设置gpu占用内存为自增长\n",
    "logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")  # 逻辑gpu\n",
    "print(f\"物理GPU数量：{len(gpus)}，逻辑GPU数量：{len(logical_gpus)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化变量\n",
    "# 训练常量\n",
    "batch_size = 64\n",
    "image_h = 48\n",
    "image_w = 48\n",
    "image_target_h = 48\n",
    "image_target_w = 48\n",
    "seed = 7\n",
    "class_num = 7\n",
    "epochs = 200\n",
    "data_path = r\"pickle_dataset\"\n",
    "# 固定随机种子\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "def scheduler(epoch):\n",
    "    # 前5个epoch学习率保持不变，5个epoch后学习率按比例衰减\n",
    "    if epoch < 10:\n",
    "        return 0.01\n",
    "    elif epoch < 100:\n",
    "        return 0.001\n",
    "    elif epoch < 150:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将全部数据读入进来，然后使用sklearn的函数进行拆分\n",
    "with open(os.path.join(data_path, \"ori_data_simple_static_strong_lbp.pickle\"), 'rb') as fp:\n",
    "    data, lbp_data, label = pickle.load(fp)\n",
    "print(\"已读入\")\n",
    "\n",
    "# 统一转换\n",
    "data = [tf.constant(i) / 255 for i in data]\n",
    "lbp_data = [tf.constant(i) / 255 for i in lbp_data]\n",
    "label = [tf.constant(i) for i in label]\n",
    "\n",
    "# 分割数据\n",
    "lst = list(range(len(data)))\n",
    "train_lst, valid_lst, train_lst, valid_lst  = train_test_split(lst, lst, test_size=0.2, random_state=seed, stratify=label)\n",
    "train_data = [data[i] for i in train_lst]\n",
    "train_data_lbp = [lbp_data[i] for i in train_lst]\n",
    "train_label = [label[i] for i in train_lst]\n",
    "valid_test_data = [data[i] for i in valid_lst]\n",
    "valid_test_data_lbp= [lbp_data[i] for i in valid_lst]\n",
    "valid_test_label = [label[i] for i in valid_lst]\n",
    "\n",
    "lst = list(range(len(valid_test_data)))\n",
    "valid_lst, test_lst, valid_lst, test_lst  = train_test_split(lst, lst, test_size=0.5, random_state=seed, stratify=valid_test_label)\n",
    "valid_data = [valid_test_data[i] for i in valid_lst]\n",
    "valid_data_lbp = [valid_test_data_lbp[i] for i in valid_lst]\n",
    "valid_label = [valid_test_label[i] for i in valid_lst]\n",
    "test_data = [valid_test_data[i] for i in test_lst]\n",
    "test_data_lbp = [valid_test_data_lbp[i] for i in test_lst]\n",
    "test_label = [valid_test_label[i] for i in test_lst]\n",
    "\n",
    "print(f\"可用数据量：trian:{len(train_data)}, valid:{len(valid_data)}, test:{len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "    target = tf.one_hot(label, class_num)\n",
    "    return image, target\n",
    "\n",
    "\n",
    "# 将数据处理称Dataset对象\n",
    "time1 = time.time()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_data, train_data_lbp), train_label)).map(augment_image,\n",
    "                                                                                    num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .shuffle(50000, seed=seed).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "print(f\"训练数据集train_dataset准备完毕, 用时：{time.time() - time1:.2f}s\")\n",
    "\n",
    "time1 = time.time()\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(((valid_data, valid_data_lbp), valid_label)).map(augment_image,\n",
    "                                                                                    num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .batch(batch_size)\n",
    "print(f\"测试数据集valid_dataset准备完毕, 用时{time.time() - time1:.2f}s\")\n",
    "\n",
    "time1 = time.time()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(((test_data, test_data_lbp), test_label)).map(augment_image,\n",
    "                                                                                    num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .batch(batch_size)\n",
    "print(f\"测试数据集test_dataset准备完毕, 用时{time.time() - time1:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict()\n",
    "for _, j in train_dataset:\n",
    "    for i in tf.argmax(j, axis=1).numpy():\n",
    "        if temp.get(i) is None:\n",
    "            temp[i] = 1\n",
    "        else:\n",
    "            temp[i] += 1\n",
    "print(temp)\n",
    "temp = dict()\n",
    "for _, j in valid_dataset:\n",
    "    for i in tf.argmax(j, axis=1).numpy():\n",
    "        if temp.get(i) is None:\n",
    "            temp[i] = 1\n",
    "        else:\n",
    "            temp[i] += 1\n",
    "print(temp)\n",
    "temp = dict()\n",
    "for _, j in test_dataset:\n",
    "    for i in tf.argmax(j, axis=1).numpy():\n",
    "        if temp.get(i) is None:\n",
    "            temp[i] = 1\n",
    "        else:\n",
    "            temp[i] += 1\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model.build(input_shape=[(None, 48, 48, 1), (None, 48, 48, 1)])\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, model_sign, data_sign):\n",
    "    # 训练\n",
    "    # tensorboard\n",
    "    dir_path = os.path.join(\"./train_results\",\n",
    "                            f\"train_result_sign_{model_sign}_{data_sign}\")\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    cnt = 0\n",
    "    for i in os.listdir(dir_path):\n",
    "        cnt = max(cnt, int(i))\n",
    "    cnt += 1\n",
    "    os.mkdir(os.path.join(dir_path, str(cnt)))\n",
    "    tensor_board_dir = os.path.join(dir_path, str(cnt), f\"tensor_board_seed_{seed}\")\n",
    "    model_check_point_dir = os.path.join(dir_path, str(cnt), f\"model_check_point_seed_{seed}\")\n",
    "    callback = [\n",
    "        callbacks.TensorBoard(tensor_board_dir),\n",
    "        callbacks.ModelCheckpoint(filepath=model_check_point_dir, save_best_only=True, monitor=\"val_accuracy\"),\n",
    "        callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=10, mode='auto', factor=0.7),\n",
    "        # callbacks.LearningRateScheduler(scheduler),\n",
    "        callbacks.EarlyStopping(monitor='loss', patience=20),\n",
    "    ]\n",
    "    print()\n",
    "    # 开始训练\n",
    "    history = model.fit(train_dataset, epochs=epochs, validation_data=valid_dataset, callbacks=callback)  # , shuffle=False, workers=1\n",
    "    print(\"训练完毕\")\n",
    "    # 评估\n",
    "    model = keras.models.load_model(model_check_point_dir)\n",
    "    score = model.evaluate(test_dataset)\n",
    "    print(f\"最优模型评估分数：{score}\")\n",
    "    # 模型保存\n",
    "    model.save_weights(\n",
    "        os.path.join(dir_path, str(cnt), f\"model_weights_{time.strftime('%Y_%m_%d')}_seed_{seed}.h5\"))\n",
    "    with open(os.path.join(\"测试结果.txt\"), 'a') as fp:\n",
    "        fp.write(f\"{model_sign}_{data_sign}_seed_{seed}\" + str(score) + \"\\n\")\n",
    "    return history.history, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history, score = train(model, \"expert_5_lbp\", \"static_strong_fer2013\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
